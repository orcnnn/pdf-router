model_name: "Qwen/Qwen2.5-VL-72B-Instruct"
ds_name: "orcn/predictions"
output_ds_name: "sghosts/processed"

# işleme seçenekleri
use_vlm: false
use_marker: true
debug: false

# streaming = indirildikçe işle (tam istediğin)
streaming: true            # indirildikçe işle
push_while_streaming: true # her chunk'ı anında pushla
skip_existing: false       # tüm split'leri işle
push_mode: append          # chunk bazlı incremental push
limit: null                # tüm satırları işle (ya da kaldır)

# performans/ram ayarları
vlm_batch_size: 8        # VLM'e mini-batch
buffer_size: 256         # RAM'de geçici sonuç tamponu
tensor_parallel_size: 4  # 2 GPU kullanıyorsan 2; tek GPU ise 1 yap
gpu_memory_utilization: 0.8
max_model_len: 16384
chunk_split_naming: timestamp
# isteğe bağlı:
# start_from_split: 0     # ya da "config-adi"
# until_split: null
#n_proc: 8               # streaming=True iken num_proc kullanılmaz (HF kısıtı)

