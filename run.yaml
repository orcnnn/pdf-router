model_name: "Qwen/Qwen2.5-VL-32B-Instruct"
ds_name: "orcn/predictions"
output_ds_name: "sghosts/orcun_processed"

# iÅŸleme seÃ§enekleri
use_vlm: false
use_marker: true
debug: true

# streaming = indirildikÃ§e iÅŸle (tam istediÄŸin)
streaming: true          # ğŸ”´ kritik
skip_existing: false     # TEST: tÃ¼m split'leri iÅŸle
push_mode: overwrite     # split bitince tek commit
limit: 10                # TEST: sadece 10 sample iÅŸle

# performans/ram ayarlarÄ±
vlm_batch_size: 8        # VLM'e mini-batch
buffer_size: 256         # RAM'de geÃ§ici sonuÃ§ tamponu
tensor_parallel_size: 4  # 2 GPU kullanÄ±yorsan 2; tek GPU ise 1 yap
gpu_memory_utilization: 0.8
max_model_len: 32000

# isteÄŸe baÄŸlÄ±:
# start_from_split: 0     # ya da "config-adi"
# until_split: null
# n_proc: 8               # streaming=True iken num_proc kullanÄ±lmaz (HF kÄ±sÄ±tÄ±)

