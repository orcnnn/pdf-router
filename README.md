# PDF Router - AkÄ±llÄ± PDF Ä°ÅŸleme Sistemi

Bu proje, PDF sayfalarÄ±nÄ± iÃ§erik tÃ¼rÃ¼ne gÃ¶re otomatik olarak sÄ±nÄ±flandÄ±rÄ±p, uygun iÅŸleme modÃ¼llerine yÃ¶nlendiren akÄ±llÄ± bir sistemdir. Marker ve Qwen2.5-VL-32B gibi farklÄ± AI modellerini kullanarak PDF iÃ§eriÄŸini optimize edilmiÅŸ ÅŸekilde iÅŸler.

## ğŸš€ Ã–zellikler

- **AkÄ±llÄ± SÄ±nÄ±flandÄ±rma**: PDF sayfalarÄ±nÄ± iÃ§erik tÃ¼rÃ¼ne gÃ¶re otomatik sÄ±nÄ±flandÄ±rÄ±r
- **Ã‡oklu Ä°ÅŸleme ModÃ¼lÃ¼**: Marker (tablo/soru/metin) ve Qwen2.5-VL (gÃ¶rsel iÃ§erik) desteÄŸi
- **Load Balancing**: Marker servisleri iÃ§in round-robin yÃ¼k dengeleme
- **Streaming Ä°ÅŸleme**: BÃ¼yÃ¼k veri setlerini bellek dostu ÅŸekilde iÅŸler
- **Hugging Face Entegrasyonu**: SonuÃ§larÄ± otomatik olarak HF Hub'a yÃ¼kler
- **Hata ToleransÄ±**: Robust hata yÃ¶netimi ve fallback mekanizmalarÄ±

## ğŸ“‹ Gereksinimler

### Sistem Gereksinimleri
- Python 3.8+
- CUDA destekli GPU (Marker ve vLLM iÃ§in)
- En az 4GB GPU RAM (Ã¶nerilen: 8GB+)
- Yeterli disk alanÄ± (geÃ§ici dosyalar iÃ§in)

### Python Paketleri
```bash
pip install -r requirements.txt
```

Ana baÄŸÄ±mlÄ±lÄ±klar:
- `datasets` - Hugging Face veri setleri
- `vllm` - VLM modeli Ã§alÄ±ÅŸtÄ±rma
- `openai` - API istemcisi
- `requests` - HTTP istekleri
- `loguru` - Loglama
- `PIL` - GÃ¶rÃ¼ntÃ¼ iÅŸleme

## ğŸ—ï¸ Kurulum

1. **Repository'yi klonlayÄ±n:**
```bash
git clone <repository-url>
cd pdf-router
```

2. **Gerekli paketleri yÃ¼kleyin:**
```bash
pip install -r requirements.txt
```

3. **Environment deÄŸiÅŸkenlerini ayarlayÄ±n:**
```bash
# .env dosyasÄ± oluÅŸturun
echo "HF_TOKEN=your_huggingface_token" >> .env
echo "VLLM_API_URL=http://localhost:8000" >> .env
echo "VLLM_API_KEY=your_api_key" >> .env
```

4. **Marker servislerini baÅŸlatÄ±n:**
```bash
# 4 farklÄ± portta marker servisleri baÅŸlatÄ±r
sh start_marker_1.sh
```

5. **vLLM servisini baÅŸlatÄ±n:**
```bash
# Slurm ortamÄ±nda
sh start_vllm_server.sh

# Veya doÄŸrudan
sh start_vllm.sh
```

## âš™ï¸ KonfigÃ¼rasyon

### `run.yaml` DosyasÄ±
```yaml
model_name: "Qwen/Qwen2.5-VL-32B-Instruct"
ds_name: "orcn/predictions"                    # GiriÅŸ veri seti
output_ds_name: "sghosts/orcun_processed"      # Ã‡Ä±kÄ±ÅŸ veri seti

# Ä°ÅŸleme seÃ§enekleri
use_vlm: true                                  # VLM kullan
use_marker: true                               # Marker kullan
debug: false                                   # Debug modu

# Streaming ve performans
streaming: true                                # Streaming modu
skip_existing: true                            # Mevcut split'leri atla
push_mode: overwrite                           # Ã‡Ä±kÄ±ÅŸ modu (overwrite/append)
vlm_batch_size: 8                             # VLM batch boyutu
buffer_size: 256                              # RAM buffer boyutu

# GPU ayarlarÄ±
tensor_parallel_size: 4                       # GPU sayÄ±sÄ±
gpu_memory_utilization: 0.8                   # GPU bellek kullanÄ±mÄ±
max_model_len: 32000                          # Maksimum model uzunluÄŸu
```

## ğŸš€ KullanÄ±m

### Temel KullanÄ±m
```bash
python main.py run.yaml
```

### TÃ¼m Servisleri BaÅŸlatma
```bash
# Marker + vLLM + Ana iÅŸlemi baÅŸlatÄ±r
sh run_job.sh
```

### Manuel Servis BaÅŸlatma
```bash
# 1. Marker servisleri
sh start_marker_1.sh

# 2. vLLM servisi
sh start_vllm_server.sh

# 3. Ana iÅŸlem
python main.py run.yaml
```

## ğŸ“Š Ä°ÅŸleme AkÄ±ÅŸÄ±

```mermaid
graph TD
    A[PDF SayfalarÄ±] --> B[Ä°Ã§erik SÄ±nÄ±flandÄ±rma]
    B --> C{Ä°Ã§erik TÃ¼rÃ¼?}
    C -->|Tablo/Soru/Metin| D[Marker Ä°ÅŸleme]
    C -->|GÃ¶rsel/Resim| E[Qwen2.5-VL Ä°ÅŸleme]
    C -->|DiÄŸer| F[Atla]
    D --> G[Metin Post-Processing]
    E --> H[VLM Post-Processing]
    G --> I[Hugging Face Hub]
    H --> I
```

## ğŸ·ï¸ Ä°Ã§erik SÄ±nÄ±flandÄ±rmasÄ±

Sistem aÅŸaÄŸÄ±daki iÃ§erik tÃ¼rlerini tanÄ±r:

| SÄ±nÄ±f | Ä°ÅŸleme ModÃ¼lÃ¼ | AÃ§Ä±klama |
|-------|---------------|----------|
| `Tablo` | Marker | Tablo iÃ§eriÄŸi |
| `Soru` | Marker | Soru metinleri |
| `Metin` | Marker | DÃ¼z metin iÃ§erik |
| `Resim` | Qwen2.5-VL | GÃ¶rsel iÃ§erik |
| `Resim/Tablo AÃ§Ä±klamasÄ±` | Qwen2.5-VL | GÃ¶rsel aÃ§Ä±klamalarÄ± |
| `Kapak SayfasÄ±` | Qwen2.5-VL | Kapak sayfalarÄ± |
| `Ä°Ã§indekiler` | Atla | Ä°Ã§indekiler sayfasÄ± |

## ğŸ”§ Marker KonfigÃ¼rasyonu

Marker servisleri aÅŸaÄŸÄ±daki parametrelerle Ã§alÄ±ÅŸÄ±r:

```json
{
    "filepath": "/tmp/tmpbzniuzcs.png",
    "page_range": null,
    "languages": null,
    "force_ocr": true,
    "paginate_output": false,
    "output_format": "markdown"
}
```

### Load Balancing
- 4 farklÄ± portta Ã§alÄ±ÅŸan Marker servisleri
- Round-robin yÃ¼k dengeleme
- Otomatik failover

## ğŸ“ Ã‡Ä±ktÄ± YapÄ±sÄ±

### Hugging Face Hub
- **Hedef**: `https://huggingface.co/datasets/sghosts/orcun_processed`
- **Format**: Hugging Face Datasets
- **Ä°Ã§erik**: Ä°ÅŸlenmiÅŸ metinler + metadata

### Log DosyalarÄ±
```
logs/
â”œâ”€â”€ marker_server_1.log    # Port 8001
â”œâ”€â”€ marker_server_2.log    # Port 8002
â”œâ”€â”€ marker_server_3.log    # Port 8003
â”œâ”€â”€ marker_server_4.log    # Port 8004
â””â”€â”€ vllm_server_*.log      # vLLM servisi
```

## ğŸ› ï¸ GeliÅŸmiÅŸ KullanÄ±m

### Streaming Modu
```yaml
streaming: true
```
- BÃ¼yÃ¼k veri setlerini bellek dostu iÅŸler
- Her split bittiÄŸinde anÄ±nda yÃ¼kler
- RAM kullanÄ±mÄ±nÄ± optimize eder

### Batch Ä°ÅŸleme
```yaml
streaming: false
vlm_batch_size: 8
```
- TÃ¼m veriyi bellekte tutar
- Daha hÄ±zlÄ± iÅŸleme
- Daha fazla RAM gerektirir

### Append Modu
```yaml
push_mode: append
```
- Mevcut verilerle birleÅŸtirir
- Duplicate kontrolÃ¼ yapar
- Incremental gÃ¼ncelleme

## ğŸ” Hata AyÄ±klama

### Log KontrolÃ¼
```bash
# Marker servisleri
tail -f logs/marker_server_*.log

# vLLM servisi
tail -f logs/vllm_server_*.log
```

### YaygÄ±n Sorunlar

1. **Marker servisleri baÅŸlamÄ±yor**
   - GPU bellek kontrolÃ¼ yapÄ±n
   - Port Ã§akÄ±ÅŸmasÄ± kontrolÃ¼

2. **vLLM baÄŸlantÄ± hatasÄ±**
   - `VLLM_API_URL` kontrolÃ¼
   - API key doÄŸrulamasÄ±

3. **Hugging Face yÃ¼kleme hatasÄ±**
   - `HF_TOKEN` kontrolÃ¼
   - Ä°nternet baÄŸlantÄ±sÄ±

## ğŸ“ˆ Performans Optimizasyonu

### GPU AyarlarÄ±
```yaml
tensor_parallel_size: 4        # GPU sayÄ±sÄ±na gÃ¶re ayarlayÄ±n
gpu_memory_utilization: 0.8    # Bellek kullanÄ±mÄ±nÄ± optimize edin
```

### Bellek AyarlarÄ±
```yaml
vlm_batch_size: 8              # VLM batch boyutu
buffer_size: 256               # RAM buffer boyutu
```

### Streaming vs Batch
- **Streaming**: BÃ¼yÃ¼k veri setleri iÃ§in
- **Batch**: KÃ¼Ã§Ã¼k veri setleri iÃ§in

## ğŸ¤ KatkÄ±da Bulunma

1. Fork yapÄ±n
2. Feature branch oluÅŸturun (`git checkout -b feature/amazing-feature`)
3. Commit yapÄ±n (`git commit -m 'Add amazing feature'`)
4. Push yapÄ±n (`git push origin feature/amazing-feature`)
5. Pull Request oluÅŸturun

## ğŸ“„ Lisans

Bu proje MIT lisansÄ± altÄ±nda lisanslanmÄ±ÅŸtÄ±r.

## ğŸ†˜ Destek

Sorunlar iÃ§in:
- Issue oluÅŸturun
- Log dosyalarÄ±nÄ± paylaÅŸÄ±n
- Sistem konfigÃ¼rasyonunu belirtin

## ğŸ“š Referanslar

- [Marker Documentation](https://github.com/VikParuchuri/marker)
- [vLLM Documentation](https://docs.vllm.ai/)
- [Qwen2.5-VL Model](https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct)
- [Hugging Face Datasets](https://huggingface.co/docs/datasets/)